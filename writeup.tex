\documentclass[12pt]{article}
\title{MAIS }
\author{Author}
\date{Date}
\begin{document}
\begin{titlepage}
   \begin{center}
       \vspace*{1cm}

       {\Huge MAIS 202 - Assignment 4}
    
       \vspace{0.5cm}
        Find maximum of multiple MNIST digits
            
       \vspace{1.5cm}

       \textbf{Efraim Herstic - Sheheryar Parvaz - Thomas Jiralerspong}

            
   \end{center}
\end{titlepage}
\tableofcontents
\newpage
\section{Implementation}
\subsection{Model}
(Sheheryar you probably know best)

\subsection{Dataset modifications}
We denoised our data by setting all pixels with a value smaller than 220 to 0. 
We also augmented our dataset to have 15000 instances of each class by adding random dark pixels, rotating, and shifting pre-existing instances of the classes using various tools provided in PIL and numpy.
\section{Results}
\subsection{Model v1}
Our initial shallow model with no batch normalization or dropout layers, trained using the non-augmented dataset, achieved a validation accuracy of 0.68 and a test accuracy of 0.73.
\subsection{Model v2}
Our deeper, more complex model, trained using the non-augmented dataset, achieved a validation accuracy of around 0.81, and an unknown test accuracy since we never submitted its predictions on Kaggle.

\subsection{Model v2 - 10000 instances per class}
Training our more complex model on an augmented dataset of 10000 instances per class yielded a validation accuracy of 0.90 and a test accuracy of 0.93.

\subsection{Model v2 - 15000 instances per class}
Training this same model on an augmented dataset consisting of 15000 instances per class, using Stochastic Gradient Descent as our optimizer, with a learning rate of 0.001, momentum of 0.9 and L2 regularization parameter of 0.01 yielded a validation accuracy of 0.93 and a test accuracy of 0.96, which was the best result we were able to achieve for this competition.

\subsection{Hyperparameters}
We tried varying the hyperparameters of our final model (learning rate, momentum, L2 regularization parameter), but any changes to these parameters only yielded slightly worse results, with test accuracies of around 0.95.
\section{Challenges}
One challenge we ran into was the unbalanced dataset containing many more instances of higher numbers than lower numbers. This initially caused our model to learn to constantly guess the highest number (9) and therefore not learn anything useful. 

As mentioned above, we solved this problem by augmenting our dataset to have 15000 instances of each class, by applying random transformations to the instances we already had. This also provided more data for our model to train on.

We tried augmenting our dataset even more, with 20000 instances per class, but our computers unfortunately did not have enough ram to store all these instances at once. We then tried to use Google Colab but.. (Efraim maybe write here)

We also tried to implement a CNN layer with attention to teach the model only to focus on the areas of the image containing digits, but were once again faced with memory constraints which we were unable to fix in time.
\section{Conclusion}
We are pretty satisfied with our final results for this Kaggle Competition, but we think we probably could have achieved an even higher test accuracy if we had been able to find a way to augment the dataset even more. 

However, we all learned a lot about the importance of using methods such as Dropout layers and a validation set to prevent overfitting. We also saw how important it was to have a balanced dataset, and were able to put into practice some of the techniques learned during the bootcamp to rebalance our dataset. We also witnessed the power of data augmentation techniques and saw how programmatically generating more instances for our dataset allowed us to significantly increase our test accuracy. Finally, we saw how memory constraints can affect the training of a model on a very large dataset and explored some options to get around these constraints.
\section{Contribution}

\subsection{Thomas}
I helped perfect the model's training process by splitting the training set into train and validation sets and by writing the code to display the validation loss and accuracy of the model at each epoch.

I also wrote the code to get the predictions of the model on the test set and put them into a .csv file that could be submitted. 

Finally, I wrote the code to perform the dataset augmentations and get a certain number of instances of each class by applying random transformations to instances we already had.
\end{document}

